{
  "name": "Postelma - Competitor Strategy Analysis",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "analyze-competitor",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "webhook-trigger",
      "name": "Webhook - Analyze Competitor",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [250, 400]
    },
    {
      "parameters": {
        "functionCode": "// Validate and extract competitor data\nconst body = $input.item.json.body;\n\nconst competitorId = body.competitor_id;\nconst name = body.name;\nconst instagramUrl = body.instagram_url;\nconst facebookUrl = body.facebook_url;\nconst linkedinUrl = body.linkedin_url;\nconst websiteUrl = body.website_url;\nconst userId = body.user_id;\n\nif (!competitorId || !name) {\n  throw new Error('competitor_id and name are required');\n}\n\nif (!instagramUrl && !facebookUrl && !linkedinUrl && !websiteUrl) {\n  throw new Error('At least one social media URL or website is required');\n}\n\nreturn {\n  json: {\n    competitor_id: competitorId,\n    user_id: userId,\n    name: name,\n    instagram_url: instagramUrl || null,\n    facebook_url: facebookUrl || null,\n    linkedin_url: linkedinUrl || null,\n    website_url: websiteUrl || null,\n    started_at: new Date().toISOString()\n  }\n};"
      },
      "id": "validate-input",
      "name": "Validate Input",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [450, 400]
    },
    {
      "parameters": {
        "functionCode": "// Scrape Instagram profile and posts with Puppeteer\nconst puppeteer = require('puppeteer');\n\nconst instagramUrl = $input.item.json.instagram_url;\nconst competitorName = $input.item.json.name;\n\nif (!instagramUrl) {\n  return {\n    json: {\n      platform: 'instagram',\n      available: false,\n      data: null\n    }\n  };\n}\n\nlet browser;\ntry {\n  browser = await puppeteer.launch({\n    headless: true,\n    args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage']\n  });\n\n  const page = await browser.newPage();\n  await page.setViewport({ width: 1280, height: 800 });\n  \n  // Go to Instagram profile\n  await page.goto(instagramUrl, { waitUntil: 'networkidle2', timeout: 30000 });\n  await page.waitForTimeout(3000);\n  \n  // Extract profile data\n  const profileData = await page.evaluate(() => {\n    try {\n      // Get followers, following, posts count from meta tags or page\n      const getMetaContent = (property) => {\n        const meta = document.querySelector(`meta[property=\"${property}\"]`);\n        return meta ? meta.content : null;\n      };\n      \n      // Try to find stats\n      const statsElements = document.querySelectorAll('span[class*=\"_ac2a\"] span');\n      const stats = Array.from(statsElements).map(el => el.textContent);\n      \n      // Get bio\n      const bioElement = document.querySelector('div[class*=\"_aa_c\"] > span');\n      const bio = bioElement ? bioElement.textContent : '';\n      \n      return {\n        followers: stats[1] || 'N/A',\n        following: stats[2] || 'N/A',\n        posts_count: stats[0] || 'N/A',\n        bio: bio\n      };\n    } catch (e) {\n      return { error: e.message };\n    }\n  });\n  \n  // Scroll to load some posts\n  await page.evaluate(() => window.scrollTo(0, 800));\n  await page.waitForTimeout(2000);\n  \n  // Extract recent posts\n  const posts = await page.evaluate(() => {\n    const postElements = document.querySelectorAll('article a[href*=\"/p/\"]');\n    const results = [];\n    \n    postElements.forEach((element, index) => {\n      if (index < 12) { // Get last 12 posts\n        const href = element.href;\n        const img = element.querySelector('img');\n        \n        results.push({\n          url: href,\n          image_url: img ? img.src : null\n        });\n      }\n    });\n    \n    return results;\n  });\n  \n  await browser.close();\n  \n  return {\n    json: {\n      platform: 'instagram',\n      available: true,\n      data: {\n        profile: profileData,\n        recent_posts: posts,\n        posts_count: posts.length,\n        scraped_at: new Date().toISOString()\n      }\n    }\n  };\n  \n} catch (error) {\n  if (browser) await browser.close();\n  console.error('Instagram scraping error:', error);\n  \n  return {\n    json: {\n      platform: 'instagram',\n      available: false,\n      error: error.message\n    }\n  };\n}"
      },
      "id": "scrape-instagram",
      "name": "Scrape Instagram",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [650, 200]
    },
    {
      "parameters": {
        "functionCode": "// Scrape Facebook Page with Puppeteer\nconst puppeteer = require('puppeteer');\n\nconst facebookUrl = $input.item.json.facebook_url;\n\nif (!facebookUrl) {\n  return {\n    json: {\n      platform: 'facebook',\n      available: false,\n      data: null\n    }\n  };\n}\n\nlet browser;\ntry {\n  browser = await puppeteer.launch({\n    headless: true,\n    args: ['--no-sandbox', '--disable-setuid-sandbox']\n  });\n\n  const page = await browser.newPage();\n  await page.setViewport({ width: 1280, height: 800 });\n  \n  await page.goto(facebookUrl, { waitUntil: 'networkidle2', timeout: 30000 });\n  await page.waitForTimeout(3000);\n  \n  // Extract page data\n  const pageData = await page.evaluate(() => {\n    try {\n      // Get page likes/followers\n      const likesElement = document.querySelector('[href*=\"/likes\"]');\n      const likes = likesElement ? likesElement.textContent.trim() : 'N/A';\n      \n      // Get recent posts\n      const postElements = document.querySelectorAll('[role=\"article\"]');\n      const posts = [];\n      \n      postElements.forEach((element, index) => {\n        if (index < 10) {\n          const textElement = element.querySelector('[data-ad-preview=\"message\"]');\n          const text = textElement ? textElement.textContent : '';\n          \n          posts.push({\n            text: text.substring(0, 200),\n            scraped: true\n          });\n        }\n      });\n      \n      return {\n        likes: likes,\n        recent_posts: posts,\n        posts_count: posts.length\n      };\n    } catch (e) {\n      return { error: e.message };\n    }\n  });\n  \n  await browser.close();\n  \n  return {\n    json: {\n      platform: 'facebook',\n      available: true,\n      data: {\n        ...pageData,\n        scraped_at: new Date().toISOString()\n      }\n    }\n  };\n  \n} catch (error) {\n  if (browser) await browser.close();\n  console.error('Facebook scraping error:', error);\n  \n  return {\n    json: {\n      platform: 'facebook',\n      available: false,\n      error: error.message\n    }\n  };\n}"
      },
      "id": "scrape-facebook",
      "name": "Scrape Facebook",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [650, 400]
    },
    {
      "parameters": {
        "functionCode": "// Scrape LinkedIn Company Page\nconst puppeteer = require('puppeteer');\n\nconst linkedinUrl = $input.item.json.linkedin_url;\n\nif (!linkedinUrl) {\n  return {\n    json: {\n      platform: 'linkedin',\n      available: false,\n      data: null\n    }\n  };\n}\n\nlet browser;\ntry {\n  browser = await puppeteer.launch({\n    headless: true,\n    args: ['--no-sandbox', '--disable-setuid-sandbox']\n  });\n\n  const page = await browser.newPage();\n  await page.setViewport({ width: 1280, height: 800 });\n  \n  await page.goto(linkedinUrl, { waitUntil: 'networkidle2', timeout: 30000 });\n  await page.waitForTimeout(3000);\n  \n  const companyData = await page.evaluate(() => {\n    try {\n      // Get company size\n      const sizeElement = document.querySelector('.org-top-card-summary-info-list__info-item');\n      const size = sizeElement ? sizeElement.textContent.trim() : 'N/A';\n      \n      // Get followers\n      const followersElement = document.querySelector('.org-top-card-primary-actions__follower-count');\n      const followers = followersElement ? followersElement.textContent.trim() : 'N/A';\n      \n      // Get description\n      const descElement = document.querySelector('.org-top-card-summary__tagline');\n      const description = descElement ? descElement.textContent.trim() : '';\n      \n      return {\n        company_size: size,\n        followers: followers,\n        description: description\n      };\n    } catch (e) {\n      return { error: e.message };\n    }\n  });\n  \n  await browser.close();\n  \n  return {\n    json: {\n      platform: 'linkedin',\n      available: true,\n      data: {\n        ...companyData,\n        scraped_at: new Date().toISOString()\n      }\n    }\n  };\n  \n} catch (error) {\n  if (browser) await browser.close();\n  console.error('LinkedIn scraping error:', error);\n  \n  return {\n    json: {\n      platform: 'linkedin',\n      available: false,\n      error: error.message\n    }\n  };\n}"
      },
      "id": "scrape-linkedin",
      "name": "Scrape LinkedIn",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [650, 600]
    },
    {
      "parameters": {
        "functionCode": "// Scrape Website for SEO and content analysis\nconst puppeteer = require('puppeteer');\n\nconst websiteUrl = $input.item.json.website_url;\n\nif (!websiteUrl) {\n  return {\n    json: {\n      platform: 'website',\n      available: false,\n      data: null\n    }\n  };\n}\n\nlet browser;\ntry {\n  browser = await puppeteer.launch({\n    headless: true,\n    args: ['--no-sandbox', '--disable-setuid-sandbox']\n  });\n\n  const page = await browser.newPage();\n  await page.setViewport({ width: 1280, height: 800 });\n  \n  await page.goto(websiteUrl, { waitUntil: 'networkidle2', timeout: 30000 });\n  await page.waitForTimeout(2000);\n  \n  const websiteData = await page.evaluate(() => {\n    try {\n      // Get meta tags\n      const getMetaContent = (name) => {\n        const meta = document.querySelector(`meta[name=\"${name}\"]`) || \n                     document.querySelector(`meta[property=\"${name}\"]`);\n        return meta ? meta.content : null;\n      };\n      \n      // Get title and description\n      const title = document.title;\n      const description = getMetaContent('description') || getMetaContent('og:description');\n      const keywords = getMetaContent('keywords');\n      \n      // Get main headings\n      const h1Elements = Array.from(document.querySelectorAll('h1')).map(h => h.textContent.trim());\n      const h2Elements = Array.from(document.querySelectorAll('h2')).map(h => h.textContent.trim()).slice(0, 5);\n      \n      // Get CTAs (Call-to-Actions)\n      const ctaButtons = Array.from(document.querySelectorAll('a[href], button')).slice(0, 10).map(btn => btn.textContent.trim()).filter(t => t.length > 0 && t.length < 50);\n      \n      // Try to find pricing info\n      const bodyText = document.body.textContent.toLowerCase();\n      const hasPricing = bodyText.includes('€') || bodyText.includes('$') || bodyText.includes('prix') || bodyText.includes('tarif');\n      \n      return {\n        title: title,\n        description: description,\n        keywords: keywords ? keywords.split(',').map(k => k.trim()) : [],\n        h1_headings: h1Elements,\n        h2_headings: h2Elements,\n        cta_buttons: ctaButtons,\n        has_pricing: hasPricing\n      };\n    } catch (e) {\n      return { error: e.message };\n    }\n  });\n  \n  await browser.close();\n  \n  return {\n    json: {\n      platform: 'website',\n      available: true,\n      data: {\n        ...websiteData,\n        url: websiteUrl,\n        scraped_at: new Date().toISOString()\n      }\n    }\n  };\n  \n} catch (error) {\n  if (browser) await browser.close();\n  console.error('Website scraping error:', error);\n  \n  return {\n    json: {\n      platform: 'website',\n      available: false,\n      error: error.message\n    }\n  };\n}"
      },
      "id": "scrape-website",
      "name": "Scrape Website",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [650, 800]
    },
    {
      "parameters": {
        "functionCode": "// Merge all scraped data\nconst validationData = $('Validate Input').item.json;\nconst allData = $input.all();\n\nconst scrapedData = {\n  competitor_id: validationData.competitor_id,\n  name: validationData.name,\n  instagram: null,\n  facebook: null,\n  linkedin: null,\n  website: null\n};\n\n// Merge data from all scraping nodes\nallData.forEach(item => {\n  const data = item.json;\n  if (data.platform && data.available) {\n    scrapedData[data.platform] = data.data;\n  }\n});\n\nreturn {\n  json: {\n    ...scrapedData,\n    scraped_at: new Date().toISOString()\n  }\n};"
      },
      "id": "merge-data",
      "name": "Merge Scraped Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [850, 400]
    },
    {
      "parameters": {
        "functionCode": "// Prepare data for OpenAI analysis\nconst data = $input.item.json;\n\nconst instagramInfo = data.instagram ? `\nINSTAGRAM:\n- Followers: ${data.instagram.profile.followers}\n- Following: ${data.instagram.profile.following}\n- Posts: ${data.instagram.profile.posts_count}\n- Bio: ${data.instagram.profile.bio}\n- Recent posts: ${data.instagram.posts_count} posts analyzed\n` : 'Instagram: Not available';\n\nconst facebookInfo = data.facebook ? `\nFACEBOOK:\n- Page Likes: ${data.facebook.likes}\n- Recent posts: ${data.facebook.posts_count} posts analyzed\n` : 'Facebook: Not available';\n\nconst linkedinInfo = data.linkedin ? `\nLINKEDIN:\n- Company Size: ${data.linkedin.company_size}\n- Followers: ${data.linkedin.followers}\n- Description: ${data.linkedin.description}\n` : 'LinkedIn: Not available';\n\nconst websiteInfo = data.website ? `\nWEBSITE:\n- Title: ${data.website.title}\n- Description: ${data.website.description}\n- Keywords: ${data.website.keywords ? data.website.keywords.join(', ') : 'N/A'}\n- Main Headings: ${data.website.h1_headings ? data.website.h1_headings.join(', ') : 'N/A'}\n- CTAs: ${data.website.cta_buttons ? data.website.cta_buttons.join(', ') : 'N/A'}\n- Has Pricing: ${data.website.has_pricing ? 'Yes' : 'No'}\n` : 'Website: Not available';\n\nconst analysisPrompt = `Tu es un expert en analyse concurrentielle et stratégie digitale. Analyse ces données d'un concurrent et fournis un rapport structuré.\n\nCONCURRENT: ${data.name}\n\nDONNÉES COLLECTÉES:\n${instagramInfo}\n${facebookInfo}\n${linkedinInfo}\n${websiteInfo}\n\nFournis ton analyse sous forme d'un objet JSON avec cette structure exacte:\n{\n  \"positioning\": \"En 1-2 phrases, décris leur positionnement marketing\",\n  \"content_strategy\": \"Décris leur stratégie de contenu (fréquence, formats, thèmes)\",\n  \"tone\": \"Qualifie leur ton de communication (ex: professionnel, casual, inspirant, éducatif)\",\n  \"target_audience\": \"Identifie leur audience cible\",\n  \"strengths\": [\"Force 1\", \"Force 2\", \"Force 3\"],\n  \"weaknesses\": [\"Faiblesse 1\", \"Faiblesse 2\"],\n  \"opportunities_for_us\": [\"Opportunité 1 pour nous différencier\", \"Opportunité 2\", \"Opportunité 3\"],\n  \"social_media_presence\": \"Évalue leur présence sociale (forte/moyenne/faible) et pourquoi\",\n  \"estimated_budget\": \"Estime leur budget marketing approximatif (petit/moyen/important) selon la qualité du contenu\",\n  \"key_differentiators\": [\"Ce qui les rend uniques 1\", \"Ce qui les rend uniques 2\"],\n  \"recommendations\": \"3 recommandations stratégiques pour les concurrencer\",\n  \"summary\": \"Résumé exécutif en 3-4 phrases\"\n}\n\nRéponds UNIQUEMENT avec le JSON, sans texte avant ou après.`;\n\nreturn {\n  json: {\n    competitor_id: data.competitor_id,\n    name: data.name,\n    raw_data: data,\n    analysis_prompt: analysisPrompt\n  }\n};"
      },
      "id": "prepare-ai-analysis",
      "name": "Prepare AI Analysis",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1050, 400]
    },
    {
      "parameters": {
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "resource": "chat",
        "model": "gpt-4o-mini",
        "messages": {
          "values": [
            {
              "role": "user",
              "content": "={{$json.analysis_prompt}}"
            }
          ]
        },
        "options": {
          "temperature": 0.3,
          "maxTokens": 2000
        }
      },
      "id": "openai-analysis",
      "name": "OpenAI Strategic Analysis",
      "type": "n8n-nodes-base.openAi",
      "typeVersion": 1,
      "position": [1250, 400],
      "credentials": {
        "openAiApi": {
          "id": "YOUR_OPENAI_CREDENTIAL_ID",
          "name": "OpenAI API"
        }
      }
    },
    {
      "parameters": {
        "functionCode": "// Parse OpenAI response and structure final analysis\nconst competitorData = $('Prepare AI Analysis').item.json;\nconst openaiResponse = $input.item.json.message.content;\n\nlet analysis;\ntry {\n  // Try to parse JSON from OpenAI\n  const jsonMatch = openaiResponse.match(/\\{[\\s\\S]*\\}/);\n  if (jsonMatch) {\n    analysis = JSON.parse(jsonMatch[0]);\n  } else {\n    throw new Error('No JSON found in response');\n  }\n} catch (e) {\n  console.error('Failed to parse OpenAI response:', e);\n  // Fallback structure\n  analysis = {\n    positioning: 'Analysis failed',\n    content_strategy: 'Could not analyze',\n    tone: 'unknown',\n    target_audience: 'unknown',\n    strengths: [],\n    weaknesses: [],\n    opportunities_for_us: [],\n    social_media_presence: 'unknown',\n    estimated_budget: 'unknown',\n    key_differentiators: [],\n    recommendations: 'Analysis incomplete',\n    summary: 'Failed to generate analysis'\n  };\n}\n\n// Calculate tokens used (rough estimate)\nconst promptTokens = competitorData.analysis_prompt.length / 4;\nconst responseTokens = openaiResponse.length / 4;\nconst totalTokens = Math.ceil(promptTokens + responseTokens);\n\n// Calculate cost (GPT-4o-mini pricing)\nconst inputCost = (promptTokens / 1000000) * 0.15; // $0.15 per 1M tokens\nconst outputCost = (responseTokens / 1000000) * 0.60; // $0.60 per 1M tokens\nconst totalCost = inputCost + outputCost;\n\nreturn {\n  json: {\n    competitor_id: competitorData.competitor_id,\n    name: competitorData.name,\n    \n    // AI Analysis\n    positioning: analysis.positioning,\n    content_strategy: analysis.content_strategy,\n    tone: analysis.tone,\n    target_audience: analysis.target_audience,\n    strengths: analysis.strengths,\n    weaknesses: analysis.weaknesses,\n    opportunities_for_us: analysis.opportunities_for_us,\n    social_media_presence: analysis.social_media_presence,\n    estimated_budget: analysis.estimated_budget,\n    key_differentiators: analysis.key_differentiators,\n    recommendations: analysis.recommendations,\n    summary: analysis.summary,\n    \n    // Raw data\n    instagram_data: competitorData.raw_data.instagram,\n    facebook_data: competitorData.raw_data.facebook,\n    linkedin_data: competitorData.raw_data.linkedin,\n    website_data: competitorData.raw_data.website,\n    \n    // Metadata\n    tokens_used: totalTokens,\n    analysis_cost: parseFloat(totalCost.toFixed(4)),\n    analyzed_at: new Date().toISOString()\n  }\n};"
      },
      "id": "parse-analysis",
      "name": "Parse Analysis Results",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1450, 400]
    },
    {
      "parameters": {
        "operation": "insert",
        "tableId": "competitor_analysis",
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "competitor_id": "={{$json.competitor_id}}",
            "positioning": "={{$json.positioning}}",
            "content_strategy": "={{$json.content_strategy}}",
            "tone": "={{$json.tone}}",
            "target_audience": "={{$json.target_audience}}",
            "strengths": "={{JSON.stringify($json.strengths)}}",
            "weaknesses": "={{JSON.stringify($json.weaknesses)}}",
            "opportunities_for_us": "={{JSON.stringify($json.opportunities_for_us)}}",
            "social_media_presence": "={{$json.social_media_presence}}",
            "estimated_budget": "={{$json.estimated_budget}}",
            "key_differentiators": "={{JSON.stringify($json.key_differentiators)}}",
            "recommendations": "={{$json.recommendations}}",
            "summary": "={{$json.summary}}",
            "instagram_data": "={{JSON.stringify($json.instagram_data)}}",
            "facebook_data": "={{JSON.stringify($json.facebook_data)}}",
            "linkedin_data": "={{JSON.stringify($json.linkedin_data)}}",
            "website_data": "={{JSON.stringify($json.website_data)}}",
            "tokens_used": "={{$json.tokens_used}}",
            "analysis_cost": "={{$json.analysis_cost}}",
            "analyzed_at": "={{$json.analyzed_at}}"
          }
        },
        "options": {}
      },
      "id": "supabase-insert-analysis",
      "name": "Save to Supabase",
      "type": "n8n-nodes-base.supabase",
      "typeVersion": 1,
      "position": [1650, 400],
      "credentials": {
        "supabaseApi": {
          "id": "YOUR_SUPABASE_CREDENTIAL_ID",
          "name": "Supabase API"
        }
      }
    },
    {
      "parameters": {
        "operation": "update",
        "tableId": "competitors",
        "filterType": "manual",
        "matchBy": "id",
        "valueToMatchBy": "={{$('Validate Input').item.json.competitor_id}}",
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "last_analyzed_at": "={{$json.analyzed_at}}",
            "instagram_followers": "={{$json.instagram_data ? $json.instagram_data.profile.followers : null}}",
            "facebook_likes": "={{$json.facebook_data ? $json.facebook_data.likes : null}}",
            "linkedin_followers": "={{$json.linkedin_data ? $json.linkedin_data.followers : null}}"
          }
        }
      },
      "id": "supabase-update-competitor",
      "name": "Update Competitor Stats",
      "type": "n8n-nodes-base.supabase",
      "typeVersion": 1,
      "position": [1650, 600],
      "credentials": {
        "supabaseApi": {
          "id": "YOUR_SUPABASE_CREDENTIAL_ID",
          "name": "Supabase API"
        }
      }
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{JSON.stringify({\n  success: true,\n  competitor_id: $json.competitor_id,\n  name: $json.name,\n  summary: $json.summary,\n  positioning: $json.positioning,\n  tone: $json.tone,\n  strengths: $json.strengths,\n  weaknesses: $json.weaknesses,\n  opportunities: $json.opportunities_for_us,\n  social_media_presence: $json.social_media_presence,\n  recommendations: $json.recommendations,\n  cost: $json.analysis_cost,\n  analyzed_at: $json.analyzed_at\n})}}",
        "options": {}
      },
      "id": "webhook-response",
      "name": "Webhook Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [1850, 400]
    }
  ],
  "connections": {
    "Webhook - Analyze Competitor": {
      "main": [
        [
          {
            "node": "Validate Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate Input": {
      "main": [
        [
          {
            "node": "Scrape Instagram",
            "type": "main",
            "index": 0
          },
          {
            "node": "Scrape Facebook",
            "type": "main",
            "index": 0
          },
          {
            "node": "Scrape LinkedIn",
            "type": "main",
            "index": 0
          },
          {
            "node": "Scrape Website",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Scrape Instagram": {
      "main": [
        [
          {
            "node": "Merge Scraped Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Scrape Facebook": {
      "main": [
        [
          {
            "node": "Merge Scraped Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Scrape LinkedIn": {
      "main": [
        [
          {
            "node": "Merge Scraped Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Scrape Website": {
      "main": [
        [
          {
            "node": "Merge Scraped Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Scraped Data": {
      "main": [
        [
          {
            "node": "Prepare AI Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare AI Analysis": {
      "main": [
        [
          {
            "node": "OpenAI Strategic Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Strategic Analysis": {
      "main": [
        [
          {
            "node": "Parse Analysis Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Analysis Results": {
      "main": [
        [
          {
            "node": "Save to Supabase",
            "type": "main",
            "index": 0
          },
          {
            "node": "Update Competitor Stats",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save to Supabase": {
      "main": [
        [
          {
            "node": "Webhook Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "tags": [],
  "triggerCount": 1,
  "updatedAt": "2025-11-10T12:00:00.000Z",
  "versionId": "1"
}
